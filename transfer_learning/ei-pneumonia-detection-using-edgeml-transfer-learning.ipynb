{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Import the data from Edge Impulse. You can obtain the URL from the Dashboard, right-click on the download icon next to 'Spectral features data' and 'Spectral features labels', and click **Copy link location**."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import numpy as np\n",
                "import requests\n",
                "\n",
                "API_KEY = 'PROVIDE YOUR 68 DIGIT API KEY FROM EDGE IMPULSE STUDIO PROJECT'\n",
                "\n",
                "X = (requests.get('https://studio.edgeimpulse.com/v1/api/PROJECT ID/training/8/x', headers={'x-api-key': API_KEY})).content\n",
                "Y = (requests.get('https://studio.edgeimpulse.com/v1/api/PROJECT ID/training/8/y', headers={'x-api-key': API_KEY})).content"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Store the data in a temporary file, and load it back through Numpy."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "with open('x_train.npy', 'wb') as file:\n",
                "    file.write(X)\n",
                "with open('y_train.npy', 'wb') as file:\n",
                "    file.write(Y)\n",
                "X = np.load('x_train.npy')\n",
                "Y = np.load('y_train.npy')[:,0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Define our labels and split the data up in a test and training set:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import sys, os, random\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "import logging\n",
                "tf.get_logger().setLevel(logging.ERROR)\n",
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "\n",
                "# Set random seeds for repeatable results\n",
                "RANDOM_SEED = 3\n",
                "random.seed(RANDOM_SEED)\n",
                "np.random.seed(RANDOM_SEED)\n",
                "tf.random.set_seed(RANDOM_SEED)\n",
                "\n",
                "classes_values = [ \"bacteria\", \"normal\", \"virus\" ]\n",
                "classes = len(classes_values)\n",
                "\n",
                "Y = tf.keras.utils.to_categorical(Y - 1, classes)\n",
                "\n",
                "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
                "\n",
                "input_length = X_train[0].shape[0]\n",
                "\n",
                "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
                "validation_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
                "\n",
                "def set_batch_size(batch_size, train_dataset, validation_dataset):\n",
                "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
                "    validation_dataset = validation_dataset.batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
                "    return train_dataset, validation_dataset\n",
                "\n",
                "callbacks = []\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Train the model:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras import Model\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Flatten, Reshape, MaxPooling1D, BatchNormalization, Conv2D, GlobalMaxPooling2D, Lambda\n",
                "from tensorflow.keras.optimizers import Adam, Adadelta\n",
                "from tensorflow.keras.losses import categorical_crossentropy\n",
                "\n",
                "INPUT_SHAPE = (96, 96, 3)\n",
                "\n",
                "base_model = tf.keras.applications.MobileNetV2(\n",
                "    input_shape=INPUT_SHAPE, alpha=0.35,\n",
                "    weights='./transfer-learning-weights/keras/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_96.h5',\n",
                "    include_top=True\n",
                ")\n",
                "base_model.trainable = False\n",
                "\n",
                "model = Sequential()\n",
                "model.add(InputLayer(input_shape=INPUT_SHAPE, name='x_input'))\n",
                "model.add(Model(inputs=base_model.inputs, outputs=base_model.layers[-3].output))\n",
                "model.add(Dense(16))\n",
                "model.add(Dropout(0.1))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(classes, activation='softmax'))\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
                "                loss='categorical_crossentropy',\n",
                "                metrics=['accuracy'])\n",
                "\n",
                "BATCH_SIZE = 32\n",
                "train_dataset, validation_dataset = set_batch_size(BATCH_SIZE, train_dataset, validation_dataset)\n",
                "\n",
                "# Set the data to the expected input shape\n",
                "def reshape(image, label):\n",
                "    return tf.reshape(image, INPUT_SHAPE), label\n",
                "train_dataset = train_dataset.map(reshape, tf.data.experimental.AUTOTUNE)\n",
                "validation_dataset = validation_dataset.map(reshape, tf.data.experimental.AUTOTUNE)\n",
                "model.fit(train_dataset, validation_data=validation_dataset, epochs=10, verbose=2, callbacks=callbacks)\n",
                "\n",
                "print('')\n",
                "print('Initial training done.', flush=True)\n",
                "\n",
                "# How many epochs we will fine tune the model\n",
                "FINE_TUNE_EPOCHS = 10\n",
                "# What percentage of the base model's layers we will fine tune\n",
                "FINE_TUNE_PERCENTAGE = 65\n",
                "\n",
                "print('Fine-tuning model for {} epochs...'.format(FINE_TUNE_EPOCHS), flush=True)\n",
                "\n",
                "# Determine which layer to begin fine tuning at\n",
                "model_layer_count = len(model.layers)\n",
                "fine_tune_from = math.ceil(model_layer_count * ((100 - FINE_TUNE_PERCENTAGE) / 100))\n",
                "\n",
                "# Allow the entire base model to be trained\n",
                "model.trainable = True\n",
                "# Freeze all the layers before the 'fine_tune_from' layer\n",
                "for layer in model.layers[:fine_tune_from]:\n",
                "    layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000045),\n",
                "                loss='categorical_crossentropy',\n",
                "                metrics=['accuracy'])\n",
                "\n",
                "model.fit(train_dataset,\n",
                "                epochs=FINE_TUNE_EPOCHS,\n",
                "                verbose=2,\n",
                "                validation_data=validation_dataset,\n",
                "                callbacks=callbacks)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "outputs": [],
            "execution_count": null,
            "source": [
                "# Save the model to disk\n",
                "model.save('saved_model')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
